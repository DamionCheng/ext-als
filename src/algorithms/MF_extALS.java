package algorithms;

import data_structure.Rating;
import data_structure.SparseMatrix;
import data_structure.DenseVector;
import data_structure.DenseMatrix;
import data_structure.Pair;
import data_structure.SparseVector;
import happy.coding.math.Randoms;

import java.util.ArrayList;
import java.util.Collections;
import java.util.Random;

import algorithms.TopKRecommender;

import java.util.HashMap;

import utils.Printer;

/**
 * FastALS for factorized weighted matrix.
 * 
 * @author Xiaoyu Du
 */
public class MF_extALS extends TopKRecommender {
	/** Model priors to set. */
	int factors = 10; // number of latent factors.
	int relates = 10; // dimensions of negative weights, i.e. Z in paper
	int maxIter = 500; // maximum iterations.
	double reg = 0.01; // regularization parameters
	double w0 = 1;
	double init_mean = 0; // Gaussian mean for init V
	double init_stdev = 0.01; // Gaussian std-dev for init V

	/** Model parameters to learn */
	public DenseMatrix U; // latent vectors for users
	public DenseMatrix V; // latent vectors for items

	/** Caches */
	double[][][] SU; // S^p_{tfk}
	double[][][] SV; // S^q_{tfk}
	double[] prediction_users, prediction_items;
	double[] rating_users, rating_items;
	double[] w_users, w_items;

	boolean showProgress;
	boolean showLoss;

	// weight for each positive instance in trainMatrix
	SparseMatrix W;

	// weight for negative instances on user u and item i with dimension T. The
	// weight on item i of user u is Wu[u].dot(Wi[i]).
	public DenseMatrix Wu, Wi;

	// weight of new instance in online learning
	public double w_new = 1;

	public MF_extALS(SparseMatrix trainMatrix, ArrayList<Rating> testRatings, int topK, int threadNum, int factors,
			int relates, int maxIter, double w0, double alpha, double reg, double init_mean, double init_stdev,
			boolean showProgress, boolean showLoss) {
		super(trainMatrix, testRatings, topK, threadNum);
		this.factors = factors;
		this.relates = relates;
		this.maxIter = maxIter;
		this.w0 = w0;
		this.reg = reg;
		this.init_mean = init_mean;
		this.init_stdev = init_stdev;
		this.showLoss = showLoss;
		this.showProgress = showProgress;

		// Set the Wi as a decay function w0 * pi ^ alpha
		double sum = 0, Z = 0;
		double[] p = new double[itemCount];
		for (int i = 0; i < itemCount; i++) {
			p[i] = trainMatrix.getColRef(i).itemCount();
			sum += p[i];
		}
		// convert p[i] to probability
		for (int i = 0; i < itemCount; i++) {
			p[i] /= sum;
			p[i] = Math.pow(p[i], alpha);
			Z += p[i];
		}
		// assign weight Wu Wi
		Wu = new DenseMatrix(userCount, relates);
		Wi = new DenseMatrix(itemCount, relates);
		for (int u = 0; u < userCount; u++) {
			for (int t = 0; t < relates; t++) {
				Wu.set(u, t, 1);
			}
		}
		for (int i = 0; i < itemCount; i++) {
			for (int t = 0; t < relates; t++) {
				Wi.set(i, t, w0 * p[i] / Z);
			}
		}
		/*
		 * Wi = new double[itemCount]; for (int i = 0; i < itemCount; i++) Wi[i]
		 * = w0 * p[i] / Z;
		 */

		// By default, the weight for positive instance is uniformly 1.
		W = new SparseMatrix(userCount, itemCount);
		for (int u = 0; u < userCount; u++)
			for (int i : trainMatrix.getRowRef(u).indexList())
				W.setValue(u, i, 1);

		// Init caches
		SU = new double[relates][factors][factors];
		SV = new double[relates][factors][factors];
		prediction_users = new double[userCount];
		prediction_items = new double[itemCount];
		rating_users = new double[userCount];
		rating_items = new double[itemCount];
		w_users = new double[userCount];
		w_items = new double[itemCount];

		// Init model parameters
		U = new DenseMatrix(userCount, factors);
		V = new DenseMatrix(itemCount, factors);
		// this initialization is changed by Xiaoyu Du temporally, to ensure the
		// same initialization.
		U.init(init_mean, init_stdev);
		V.init(init_mean, init_stdev);
		initS();
	}

	public void setTrain(SparseMatrix trainMatrix) {
		this.trainMatrix = new SparseMatrix(trainMatrix);
		W = new SparseMatrix(userCount, itemCount);
		for (int u = 0; u < userCount; u++)
			for (int i : this.trainMatrix.getRowRef(u).indexList())
				W.setValue(u, i, 1);
	}

	// Init SU and SV
	private void initS() {

		// clear SV SU
		for (int k = 0; k < factors; k++) {
			for (int f = 0; f < factors; f++) {
				for (int t = 0; t < relates; t++) {
					SU[t][k][f] = 0;
					SV[t][k][f] = 0;
				}
			}
		}
		// initialize SU
		for (int u = 0; u < userCount; u++) {
			for (int k = 0; k < factors; k++) {
				for (int f = 0; f < factors; f++) {
					for (int t = 0; t < relates; t++) {
						SU[t][k][f] += Wu.get(u, t) * U.get(u, k) * U.get(u, f);
					}
				}
			}
		}

		// initialize SV
		for (int i = 0; i < itemCount; i++) {
			for (int k = 0; k < factors; k++) {
				for (int f = 0; f < factors; f++) {
					for (int t = 0; t < relates; t++) {
						SV[t][k][f] += Wi.get(i, t) * V.get(i, k) * V.get(i, f);
					}
				}
			}
		}
	}

	// reset U, V
	public void setUV(DenseMatrix U, DenseMatrix V) {
		this.U = U.clone();
		this.V = V.clone();
		initS();
	}

	public void buildModel() {
		System.out.printf("user: %d, item: %d\n", userCount, itemCount);
		System.out.printf("relates: %d\n", relates);
		// System.out.println("Run for FastALS. ");
		double loss_pre = Double.MAX_VALUE;
		for (int iter = 0; iter < maxIter; iter++) {
			Long start = System.currentTimeMillis();

			// Update user latent vectors
			for (int u = 0; u < userCount; u++) {
				update_user(u);
			}
			// Update item latent vectors
			for (int i = 0; i < itemCount; i++) {
				update_item(i);
			}
			// Show progress
			if (showProgress)
				showProgress(iter, start, testRatings);
			// Show loss
			if (showLoss)
				loss_pre = showLoss(iter, start, loss_pre);
		} // end for iter
	}

	// Run model for one iteration
	public void runOneIteration() {
		// Update user latent vectors
		for (int u = 0; u < userCount; u++) {
			update_user(u);
		}

		// Update item latent vectors
		for (int i = 0; i < itemCount; i++) {
			update_item(i);
		}
	}

	protected void update_user(int u) {
		ArrayList<Integer> itemList = trainMatrix.getRowRef(u).indexList();
		if (itemList.size() == 0)
			return; // user has no ratings
		// prediction cache for the user
		for (int i : itemList) {
			prediction_items[i] = predict(u, i);
			rating_items[i] = trainMatrix.getValue(u, i);
			w_items[i] = W.getValue(u, i);
		}
		DenseVector oldVector = U.row(u);
		for (int f = 0; f < factors; f++) {
			double numer = 0, denom = 0;
			// O(KZ) complexity
			for (int k = 0; k < factors; k++) {
				if (k != f) {
					double tsum = 0;
					for (int t = 0; t < relates; t++) {
						tsum += Wu.get(u, t) * SV[t][k][f];
					}
					numer -= U.get(u, k) * tsum;
				}
			}
			// O(|\R_u|Z) complexity
			for (int i : itemList) {
				prediction_items[i] -= U.get(u, f) * V.get(i, f);
				double Wui = 0.;
				for (int t = 0; t < relates; t++) {
					Wui += Wu.get(u, t) * Wi.get(i, t);
				}
				numer += (w_items[i] * rating_items[i] - (w_items[i] - Wui) * prediction_items[i]) * V.get(i, f);
				denom += (w_items[i] - Wui) * V.get(i, f) * V.get(i, f);
			}
			// O(Z) complexity
			for (int t = 0; t < relates; t++) {
				denom += Wu.get(u, t) * SV[t][f][f];
			}
			denom += reg;

			// Parameter Update
			U.set(u, f, numer / denom);

			// Update the prediction cache
			//  O(|\R_u|Z) Complexity
			for (int i : itemList)
				prediction_items[i] += U.get(u, f) * V.get(i, f);
		} // end for f

		// Update the SU cache
		for (int t = 0; t < relates; t++) {
			for (int f = 0; f < factors; f++) {
				for (int k = 0; k <= f; k++) {
					double val = SU[t][f][k]
							- Wu.get(u, t) * (oldVector.get(f) * oldVector.get(k) - U.get(u, f) * U.get(u, k));
					SU[t][f][k] = val;
					SU[t][k][f] = val;
				}
			} // end for f
		} // end for t
	}

	protected void update_item(int i) {
		ArrayList<Integer> userList = trainMatrix.getColRef(i).indexList();
		if (userList.size() == 0)
			return; // item has no ratings.
		// prediction cache for the item
		for (int u : userList) {
			prediction_users[u] = predict(u, i);
			rating_users[u] = trainMatrix.getValue(u, i);
			w_users[u] = W.getValue(u, i);
		}
		DenseVector oldVector = V.row(i);
		for (int f = 0; f < factors; f++) {
			double numer = 0, denom = 0;
			// O(KZ) complexity
			for (int k = 0; k < factors; k++) {
				if (k != f) {
					double tsum = 0.;
					for (int t = 0; t < relates; t++) {
						tsum += Wi.get(i, t) * SU[t][k][f];
					}
					numer -= V.get(i, k) * tsum;
				}
			}

			// O(|\R_i|Z) Complexity
			for (int u : userList) {
				prediction_users[u] -= U.get(u, f) * V.get(i, f);
				// DenseVector wu = Wu.row(u);
				double Wui = 0.;
				for (int t = 0; t < relates; t++) {
					Wui += Wu.get(u, t) * Wi.get(i, t);
				}
				numer += (w_users[u] * rating_users[u] - (w_users[u] - Wui) * prediction_users[u]) * U.get(u, f);
				denom += (w_users[u] - Wui) * U.get(u, f) * U.get(u, f);
			}
			// O(Z) Complexity
			for (int t = 0; t < relates; t++) {
				denom += Wi.get(i, t) * SU[t][f][f];
			}
			denom += reg;

			// Parameter update
			V.set(i, f, numer / denom);
			// Update the prediction cache for the item
			// O(|\R_i|Z) Complexity
			for (int u : userList)
				prediction_users[u] += U.get(u, f) * V.get(i, f);
		} // end for f

		// Update the SV cache
		for (int t = 0; t < relates; t++) {
			for (int f = 0; f < factors; f++) {
				for (int k = 0; k <= f; k++) {
					double val = SV[t][f][k]
							- Wi.get(i, t) * (oldVector.get(f) * oldVector.get(k) - V.get(i, f) * V.get(i, k));
					SV[t][f][k] = val;
					SV[t][k][f] = val;
				}
			}
		}
	}

	public double showLoss(int iter, long start, double loss_pre) {
		long start1 = System.currentTimeMillis();
		double loss_cur = loss();
		String symbol = loss_pre >= loss_cur ? "-" : "+";
		System.out.printf("Iter=%d [%s]\t [%s]loss: %.4f [%s]\n", iter, Printer.printTime(start1 - start), symbol,
				loss_cur, Printer.printTime(System.currentTimeMillis() - start1));
		return loss_cur;
	}

	// Fast way to calculate the loss function
	public double loss() {
		double L = reg * (U.squaredSum() + V.squaredSum());
		for (int u = 0; u < userCount; u++) {
			double l = 0;
			for (int i : trainMatrix.getRowRef(u).indexList()) {
				double pred = predict(u, i);
				// DenseVector wi = Wi.row(i);
				double Wui = 0.;
				for (int t = 0; t < relates; t++) {
					Wui += Wu.get(u, t) * Wi.get(i, t);
				}
				l += W.getValue(u, i) * Math.pow(trainMatrix.getValue(u, i) - pred, 2);
				l -= Wui * Math.pow(pred, 2);
			}

			for (int t = 0; t < relates; t++) {
				double s2 = 0.;
				for (int f = 0; f < factors; f++) {
					double s1 = 0.;
					for (int k = 0; k < factors; k++) {
						s1 += U.get(u, k) * SV[t][f][k];
					}
					s2 += U.get(u, f) * s1;
				}
				l += Wu.get(u, t) * s2;
			}
			L += l;
		}
		return L;
	}

	@Override
	public double predict(int u, int i) {
		return U.row(u, false).inner(V.row(i, false));
	}

	@Override
	public void updateModel(int u, int i) {
		// not implemented
	}

}
/*

HoldOne out splitting.
Sort items for each user.[00:00:00.470]
Generate rating matrices.[00:00:01.056]
Data	data/yelp.rating
#Users	25677
#Items	25815
#Ratings	 672765 (train), 25677(test)
extALS: showProgress=false, factors=64, maxIter=500, reg=0.010000, w0=10.00, alpha=0.75
====================================================
Popularity	 <hr, ndcg, prec>:	 0.0705	 0.0176	 0.0062 [00:00:02.233]
training size: 662851755
user: 25677, item: 25815
relates: 1
Iter=0 [00:00:06.892]	 [-]loss: 261836.1007 [00:00:00.338]
Iter=1 [00:00:04.821]	 [-]loss: 96839.4645 [00:00:00.496]
Iter=2 [00:00:04.554]	 [-]loss: 67288.0789 [00:00:00.242]
Iter=3 [00:00:04.531]	 [-]loss: 54636.3216 [00:00:00.225]
Iter=4 [00:00:04.514]	 [-]loss: 47446.8384 [00:00:00.224]
Iter=5 [00:00:04.506]	 [-]loss: 42775.1669 [00:00:00.222]
Iter=6 [00:00:04.485]	 [-]loss: 39475.7438 [00:00:00.227]
Iter=7 [00:00:04.505]	 [-]loss: 37002.0581 [00:00:00.226]
Iter=8 [00:00:04.591]	 [-]loss: 35060.2324 [00:00:00.229]
Iter=9 [00:00:04.546]	 [-]loss: 33479.9425 [00:00:00.229]
Iter=10 [00:00:04.559]	 [-]loss: 32157.4193 [00:00:00.228]
Iter=11 [00:00:04.613]	 [-]loss: 31027.1596 [00:00:00.231]
Iter=12 [00:00:04.572]	 [-]loss: 30046.2365 [00:00:00.233]
Iter=13 [00:00:04.538]	 [-]loss: 29185.1848 [00:00:00.224]
Iter=14 [00:00:04.557]	 [-]loss: 28422.7083 [00:00:00.237]
Iter=15 [00:00:04.564]	 [-]loss: 27742.7244 [00:00:00.225]
Iter=16 [00:00:04.547]	 [-]loss: 27132.6735 [00:00:00.226]
Iter=17 [00:00:04.588]	 [-]loss: 26582.4973 [00:00:00.228]
Iter=18 [00:00:04.566]	 [-]loss: 26083.9762 [00:00:00.225]
Iter=19 [00:00:04.550]	 [-]loss: 25630.2859 [00:00:00.228]
Iter=20 [00:00:04.574]	 [-]loss: 25215.7115 [00:00:00.224]
Iter=21 [00:00:04.535]	 [-]loss: 24835.4411 [00:00:00.227]
Iter=22 [00:00:04.506]	 [-]loss: 24485.4029 [00:00:00.225]
Iter=23 [00:00:04.510]	 [-]loss: 24162.1307 [00:00:00.223]
Iter=24 [00:00:04.523]	 [-]loss: 23862.6545 [00:00:00.226]
Iter=25 [00:00:04.503]	 [-]loss: 23584.4155 [00:00:00.228]
Iter=26 [00:00:04.514]	 [-]loss: 23325.1992 [00:00:00.229]
Iter=27 [00:00:04.566]	 [-]loss: 23083.0798 [00:00:00.227]
Iter=28 [00:00:04.603]	 [-]loss: 22856.3758 [00:00:00.225]
Iter=29 [00:00:04.632]	 [-]loss: 22643.6149 [00:00:00.226]
Iter=30 [00:00:04.524]	 [-]loss: 22443.5050 [00:00:00.225]
Iter=31 [00:00:04.663]	 [-]loss: 22254.9070 [00:00:00.227]
Iter=32 [00:00:04.623]	 [-]loss: 22076.8149 [00:00:00.229]
Iter=33 [00:00:04.663]	 [-]loss: 21908.3398 [00:00:00.225]
Iter=34 [00:00:04.580]	 [-]loss: 21748.6950 [00:00:00.224]
Iter=35 [00:00:04.528]	 [-]loss: 21597.1818 [00:00:00.227]
Iter=36 [00:00:04.535]	 [-]loss: 21453.1786 [00:00:00.224]
Iter=37 [00:00:04.541]	 [-]loss: 21316.1297 [00:00:00.223]
Iter=38 [00:00:04.523]	 [-]loss: 21185.5367 [00:00:00.225]
Iter=39 [00:00:04.525]	 [-]loss: 21060.9505 [00:00:00.227]
Iter=40 [00:00:04.523]	 [-]loss: 20941.9652 [00:00:00.226]
Iter=41 [00:00:05.074]	 [-]loss: 20828.2120 [00:00:00.334]
Iter=42 [00:00:04.910]	 [-]loss: 20719.3558 [00:00:00.237]
Iter=43 [00:00:04.868]	 [-]loss: 20615.0902 [00:00:00.225]
Iter=44 [00:00:04.525]	 [-]loss: 20515.1353 [00:00:00.224]
Iter=45 [00:00:04.791]	 [-]loss: 20419.2337 [00:00:00.238]
Iter=46 [00:00:04.867]	 [-]loss: 20327.1489 [00:00:00.417]
Iter=47 [00:00:04.936]	 [-]loss: 20238.6622 [00:00:00.239]
Iter=48 [00:00:04.645]	 [-]loss: 20153.5715 [00:00:00.225]
Iter=49 [00:00:04.534]	 [-]loss: 20071.6890 [00:00:00.222]
Iter=50 [00:00:04.689]	 [-]loss: 19992.8403 [00:00:00.250]
Iter=51 [00:00:04.882]	 [-]loss: 19916.8628 [00:00:00.322]
Iter=52 [00:00:04.776]	 [-]loss: 19843.6050 [00:00:00.244]
Iter=53 [00:00:04.818]	 [-]loss: 19772.9253 [00:00:00.240]
Iter=54 [00:00:04.820]	 [-]loss: 19704.6913 [00:00:00.225]
Iter=55 [00:00:04.790]	 [-]loss: 19638.7789 [00:00:00.227]
Iter=56 [00:00:04.571]	 [-]loss: 19575.0719 [00:00:00.253]
Iter=57 [00:00:04.769]	 [-]loss: 19513.4613 [00:00:00.314]
Iter=58 [00:00:04.753]	 [-]loss: 19453.8449 [00:00:00.238]
Iter=59 [00:00:04.912]	 [-]loss: 19396.1266 [00:00:00.232]
Iter=60 [00:00:05.110]	 [-]loss: 19340.2162 [00:00:00.246]
Iter=61 [00:00:04.720]	 [-]loss: 19286.0290 [00:00:00.226]
Iter=62 [00:00:04.732]	 [-]loss: 19233.4853 [00:00:00.231]
Iter=63 [00:00:04.784]	 [-]loss: 19182.5101 [00:00:00.228]
Iter=64 [00:00:04.760]	 [-]loss: 19133.0329 [00:00:00.234]
Iter=65 [00:00:04.780]	 [-]loss: 19084.9873 [00:00:00.227]
Iter=66 [00:00:04.754]	 [-]loss: 19038.3105 [00:00:00.235]
Iter=67 [00:00:04.734]	 [-]loss: 18992.9438 [00:00:00.276]
Iter=68 [00:00:04.690]	 [-]loss: 18948.8313 [00:00:00.243]
Iter=69 [00:00:04.632]	 [-]loss: 18905.9206 [00:00:00.238]
Iter=70 [00:00:04.610]	 [-]loss: 18864.1622 [00:00:00.230]
Iter=71 [00:00:04.743]	 [-]loss: 18823.5089 [00:00:00.258]
Iter=72 [00:00:04.758]	 [-]loss: 18783.9166 [00:00:00.223]
Iter=73 [00:00:04.707]	 [-]loss: 18745.3430 [00:00:00.300]
Iter=74 [00:00:04.808]	 [-]loss: 18707.7481 [00:00:00.223]
Iter=75 [00:00:04.831]	 [-]loss: 18671.0940 [00:00:00.230]
Iter=76 [00:00:04.913]	 [-]loss: 18635.3447 [00:00:00.331]
Iter=77 [00:00:05.078]	 [-]loss: 18600.4658 [00:00:00.241]
Iter=78 [00:00:04.781]	 [-]loss: 18566.4247 [00:00:00.225]
Iter=79 [00:00:04.822]	 [-]loss: 18533.1906 [00:00:00.236]
Iter=80 [00:00:04.659]	 [-]loss: 18500.7338 [00:00:00.226]
Iter=81 [00:00:04.599]	 [-]loss: 18469.0264 [00:00:00.225]
Iter=82 [00:00:04.580]	 [-]loss: 18438.0416 [00:00:00.228]
Iter=83 [00:00:04.541]	 [-]loss: 18407.7543 [00:00:00.225]
Iter=84 [00:00:04.760]	 [-]loss: 18378.1401 [00:00:00.226]
Iter=85 [00:00:04.693]	 [-]loss: 18349.1762 [00:00:00.282]
Iter=86 [00:00:05.050]	 [-]loss: 18320.8408 [00:00:00.308]
Iter=87 [00:00:04.885]	 [-]loss: 18293.1130 [00:00:00.232]
Iter=88 [00:00:04.696]	 [-]loss: 18265.9733 [00:00:00.226]
Iter=89 [00:00:04.664]	 [-]loss: 18239.4026 [00:00:00.247]
Iter=90 [00:00:04.505]	 [-]loss: 18213.3829 [00:00:00.226]
Iter=91 [00:00:04.505]	 [-]loss: 18187.8971 [00:00:00.225]
Iter=92 [00:00:04.520]	 [-]loss: 18162.9285 [00:00:00.226]
Iter=93 [00:00:04.557]	 [-]loss: 18138.4613 [00:00:00.223]
Iter=94 [00:00:04.946]	 [-]loss: 18114.4802 [00:00:00.225]
Iter=95 [00:00:04.745]	 [-]loss: 18090.9703 [00:00:00.285]
Iter=96 [00:00:04.725]	 [-]loss: 18067.9176 [00:00:00.259]
Iter=97 [00:00:04.788]	 [-]loss: 18045.3084 [00:00:00.228]
Iter=98 [00:00:04.874]	 [-]loss: 18023.1294 [00:00:00.234]
Iter=99 [00:00:04.574]	 [-]loss: 18001.3682 [00:00:00.225]
Iter=100 [00:00:04.542]	 [-]loss: 17980.0124 [00:00:00.225]
Iter=101 [00:00:04.664]	 [-]loss: 17959.0506 [00:00:00.270]
Iter=102 [00:00:04.820]	 [-]loss: 17938.4716 [00:00:00.226]
Iter=103 [00:00:04.527]	 [-]loss: 17918.2647 [00:00:00.225]
Iter=104 [00:00:04.591]	 [-]loss: 17898.4195 [00:00:00.240]
Iter=105 [00:00:04.884]	 [-]loss: 17878.9264 [00:00:00.263]
Iter=106 [00:00:04.626]	 [-]loss: 17859.7758 [00:00:00.227]
Iter=107 [00:00:04.872]	 [-]loss: 17840.9588 [00:00:00.227]
Iter=108 [00:00:04.615]	 [-]loss: 17822.4665 [00:00:00.224]
Iter=109 [00:00:04.527]	 [-]loss: 17804.2907 [00:00:00.229]
Iter=110 [00:00:04.525]	 [-]loss: 17786.4234 [00:00:00.227]
Iter=111 [00:00:04.519]	 [-]loss: 17768.8568 [00:00:00.227]
Iter=112 [00:00:04.559]	 [-]loss: 17751.5835 [00:00:00.227]
Iter=113 [00:00:04.529]	 [-]loss: 17734.5963 [00:00:00.229]
Iter=114 [00:00:04.543]	 [-]loss: 17717.8884 [00:00:00.224]
Iter=115 [00:00:04.504]	 [-]loss: 17701.4530 [00:00:00.229]
Iter=116 [00:00:04.527]	 [-]loss: 17685.2838 [00:00:00.225]
Iter=117 [00:00:04.621]	 [-]loss: 17669.3745 [00:00:00.228]
Iter=118 [00:00:04.531]	 [-]loss: 17653.7191 [00:00:00.225]
Iter=119 [00:00:04.529]	 [-]loss: 17638.3119 [00:00:00.228]
Iter=120 [00:00:04.622]	 [-]loss: 17623.1473 [00:00:00.225]
Iter=121 [00:00:04.518]	 [-]loss: 17608.2199 [00:00:00.227]
Iter=122 [00:00:04.550]	 [-]loss: 17593.5245 [00:00:00.226]
Iter=123 [00:00:04.522]	 [-]loss: 17579.0561 [00:00:00.224]
Iter=124 [00:00:04.558]	 [-]loss: 17564.8097 [00:00:00.228]
Iter=125 [00:00:04.575]	 [-]loss: 17550.7807 [00:00:00.232]
Iter=126 [00:00:04.551]	 [-]loss: 17536.9645 [00:00:00.234]
Iter=127 [00:00:04.576]	 [-]loss: 17523.3565 [00:00:00.227]
Iter=128 [00:00:04.630]	 [-]loss: 17509.9524 [00:00:00.228]
Iter=129 [00:00:04.604]	 [-]loss: 17496.7481 [00:00:00.225]
Iter=130 [00:00:04.630]	 [-]loss: 17483.7393 [00:00:00.229]
Iter=131 [00:00:04.681]	 [-]loss: 17470.9220 [00:00:00.223]
Iter=132 [00:00:04.485]	 [-]loss: 17458.2923 [00:00:00.224]
Iter=133 [00:00:04.510]	 [-]loss: 17445.8465 [00:00:00.232]
Iter=134 [00:00:04.580]	 [-]loss: 17433.5807 [00:00:00.223]
Iter=135 [00:00:04.538]	 [-]loss: 17421.4913 [00:00:00.226]
Iter=136 [00:00:04.537]	 [-]loss: 17409.5747 [00:00:00.228]
Iter=137 [00:00:04.528]	 [-]loss: 17397.8275 [00:00:00.233]
Iter=138 [00:00:04.544]	 [-]loss: 17386.2463 [00:00:00.224]
Iter=139 [00:00:04.547]	 [-]loss: 17374.8278 [00:00:00.231]
Iter=140 [00:00:04.568]	 [-]loss: 17363.5687 [00:00:00.224]
Iter=141 [00:00:04.503]	 [-]loss: 17352.4660 [00:00:00.241]
Iter=142 [00:00:04.478]	 [-]loss: 17341.5167 [00:00:00.223]
Iter=143 [00:00:04.641]	 [-]loss: 17330.7178 [00:00:00.229]
Iter=144 [00:00:04.596]	 [-]loss: 17320.0665 [00:00:00.231]
Iter=145 [00:00:04.576]	 [-]loss: 17309.5604 [00:00:00.231]
Iter=146 [00:00:04.556]	 [-]loss: 17299.1967 [00:00:00.237]
Iter=147 [00:00:04.568]	 [-]loss: 17288.9732 [00:00:00.228]
Iter=148 [00:00:04.561]	 [-]loss: 17278.8873 [00:00:00.225]
Iter=149 [00:00:04.850]	 [-]loss: 17268.9367 [00:00:00.292]
Iter=150 [00:00:04.813]	 [-]loss: 17259.1190 [00:00:00.268]
Iter=151 [00:00:04.746]	 [-]loss: 17249.4317 [00:00:00.225]
Iter=152 [00:00:04.621]	 [-]loss: 17239.8726 [00:00:00.225]
Iter=153 [00:00:04.556]	 [-]loss: 17230.4392 [00:00:00.228]
Iter=154 [00:00:04.547]	 [-]loss: 17221.1294 [00:00:00.223]
Iter=155 [00:00:04.594]	 [-]loss: 17211.9408 [00:00:00.225]
Iter=156 [00:00:04.615]	 [-]loss: 17202.8711 [00:00:00.225]
Iter=157 [00:00:04.703]	 [-]loss: 17193.9182 [00:00:00.228]
Iter=158 [00:00:04.712]	 [-]loss: 17185.0795 [00:00:00.224]
Iter=159 [00:00:04.594]	 [-]loss: 17176.3529 [00:00:00.224]
Iter=160 [00:00:04.547]	 [-]loss: 17167.7362 [00:00:00.228]
Iter=161 [00:00:04.534]	 [-]loss: 17159.2273 [00:00:00.229]
Iter=162 [00:00:04.520]	 [-]loss: 17150.8240 [00:00:00.230]
Iter=163 [00:00:04.524]	 [-]loss: 17142.5244 [00:00:00.227]
Iter=164 [00:00:04.557]	 [-]loss: 17134.3267 [00:00:00.226]
Iter=165 [00:00:04.551]	 [-]loss: 17126.2289 [00:00:00.224]
Iter=166 [00:00:04.556]	 [-]loss: 17118.2294 [00:00:00.227]
Iter=167 [00:00:04.521]	 [-]loss: 17110.3264 [00:00:00.227]
Iter=168 [00:00:04.534]	 [-]loss: 17102.5182 [00:00:00.225]
Iter=169 [00:00:04.527]	 [-]loss: 17094.8032 [00:00:00.227]
Iter=170 [00:00:04.535]	 [-]loss: 17087.1799 [00:00:00.229]
Iter=171 [00:00:04.640]	 [-]loss: 17079.6468 [00:00:00.225]
Iter=172 [00:00:04.569]	 [-]loss: 17072.2023 [00:00:00.227]
Iter=173 [00:00:04.559]	 [-]loss: 17064.8450 [00:00:00.227]
Iter=174 [00:00:04.562]	 [-]loss: 17057.5735 [00:00:00.228]
Iter=175 [00:00:04.581]	 [-]loss: 17050.3865 [00:00:00.230]
Iter=176 [00:00:04.531]	 [-]loss: 17043.2825 [00:00:00.231]
Iter=177 [00:00:04.572]	 [-]loss: 17036.2602 [00:00:00.232]
Iter=178 [00:00:04.571]	 [-]loss: 17029.3184 [00:00:00.229]
Iter=179 [00:00:04.574]	 [-]loss: 17022.4558 [00:00:00.232]
Iter=180 [00:00:04.551]	 [-]loss: 17015.6712 [00:00:00.229]
Iter=181 [00:00:04.542]	 [-]loss: 17008.9633 [00:00:00.230]
Iter=182 [00:00:04.526]	 [-]loss: 17002.3310 [00:00:00.224]
Iter=183 [00:00:04.529]	 [-]loss: 16995.7730 [00:00:00.223]
Iter=184 [00:00:04.512]	 [-]loss: 16989.2882 [00:00:00.225]
Iter=185 [00:00:04.530]	 [-]loss: 16982.8755 [00:00:00.224]
Iter=186 [00:00:04.535]	 [-]loss: 16976.5336 [00:00:00.225]
Iter=187 [00:00:04.552]	 [-]loss: 16970.2615 [00:00:00.226]
Iter=188 [00:00:04.587]	 [-]loss: 16964.0581 [00:00:00.231]
Iter=189 [00:00:04.514]	 [-]loss: 16957.9222 [00:00:00.226]
Iter=190 [00:00:04.540]	 [-]loss: 16951.8529 [00:00:00.227]
Iter=191 [00:00:04.552]	 [-]loss: 16945.8491 [00:00:00.228]
Iter=192 [00:00:04.541]	 [-]loss: 16939.9096 [00:00:00.230]
Iter=193 [00:00:04.537]	 [-]loss: 16934.0337 [00:00:00.228]
Iter=194 [00:00:04.520]	 [-]loss: 16928.2201 [00:00:00.230]
Iter=195 [00:00:04.531]	 [-]loss: 16922.4680 [00:00:00.227]
Iter=196 [00:00:04.544]	 [-]loss: 16916.7765 [00:00:00.241]
Iter=197 [00:00:04.532]	 [-]loss: 16911.1445 [00:00:00.235]
Iter=198 [00:00:04.532]	 [-]loss: 16905.5712 [00:00:00.225]
Iter=199 [00:00:04.528]	 [-]loss: 16900.0557 [00:00:00.228]
Iter=200 [00:00:04.527]	 [-]loss: 16894.5971 [00:00:00.228]
Iter=201 [00:00:04.561]	 [-]loss: 16889.1945 [00:00:00.244]
Iter=202 [00:00:04.574]	 [-]loss: 16883.8470 [00:00:00.223]
Iter=203 [00:00:04.539]	 [-]loss: 16878.5539 [00:00:00.231]
Iter=204 [00:00:04.509]	 [-]loss: 16873.3143 [00:00:00.221]
Iter=205 [00:00:04.633]	 [-]loss: 16868.1274 [00:00:00.231]
Iter=206 [00:00:04.570]	 [-]loss: 16862.9925 [00:00:00.231]
Iter=207 [00:00:04.552]	 [-]loss: 16857.9087 [00:00:00.240]
Iter=208 [00:00:04.539]	 [-]loss: 16852.8754 [00:00:00.224]
Iter=209 [00:00:04.521]	 [-]loss: 16847.8917 [00:00:00.229]
Iter=210 [00:00:04.689]	 [-]loss: 16842.9570 [00:00:00.222]
Iter=211 [00:00:04.589]	 [-]loss: 16838.0706 [00:00:00.233]
Iter=212 [00:00:04.571]	 [-]loss: 16833.2319 [00:00:00.226]
Iter=213 [00:00:04.551]	 [-]loss: 16828.4401 [00:00:00.228]
Iter=214 [00:00:04.532]	 [-]loss: 16823.6946 [00:00:00.224]
Iter=215 [00:00:04.567]	 [-]loss: 16818.9948 [00:00:00.225]
Iter=216 [00:00:04.789]	 [-]loss: 16814.3401 [00:00:00.285]
Iter=217 [00:00:04.964]	 [-]loss: 16809.7299 [00:00:00.225]
Iter=218 [00:00:04.808]	 [-]loss: 16805.1636 [00:00:00.244]
Iter=219 [00:00:04.625]	 [-]loss: 16800.6406 [00:00:00.225]
Iter=220 [00:00:04.532]	 [-]loss: 16796.1605 [00:00:00.262]
Iter=221 [00:00:04.498]	 [-]loss: 16791.7225 [00:00:00.224]
Iter=222 [00:00:04.541]	 [-]loss: 16787.3263 [00:00:00.225]
Iter=223 [00:00:04.534]	 [-]loss: 16782.9712 [00:00:00.227]
Iter=224 [00:00:04.522]	 [-]loss: 16778.6567 [00:00:00.226]
Iter=225 [00:00:04.532]	 [-]loss: 16774.3824 [00:00:00.229]
Iter=226 [00:00:04.546]	 [-]loss: 16770.1477 [00:00:00.223]
Iter=227 [00:00:04.701]	 [-]loss: 16765.9520 [00:00:00.224]
Iter=228 [00:00:04.597]	 [-]loss: 16761.7950 [00:00:00.228]
Iter=229 [00:00:04.663]	 [-]loss: 16757.6760 [00:00:00.231]
Iter=230 [00:00:04.633]	 [-]loss: 16753.5947 [00:00:00.239]
Iter=231 [00:00:04.671]	 [-]loss: 16749.5506 [00:00:00.237]
Iter=232 [00:00:04.645]	 [-]loss: 16745.5431 [00:00:00.230]
Iter=233 [00:00:04.659]	 [-]loss: 16741.5720 [00:00:00.230]
Iter=234 [00:00:04.656]	 [-]loss: 16737.6366 [00:00:00.224]
Iter=235 [00:00:04.584]	 [-]loss: 16733.7367 [00:00:00.244]
Iter=236 [00:00:04.602]	 [-]loss: 16729.8719 [00:00:00.232]
Iter=237 [00:00:04.535]	 [-]loss: 16726.0416 [00:00:00.225]
Iter=238 [00:00:04.530]	 [-]loss: 16722.2457 [00:00:00.232]
Iter=239 [00:00:04.551]	 [-]loss: 16718.4836 [00:00:00.225]
Iter=240 [00:00:04.540]	 [-]loss: 16714.7551 [00:00:00.227]
Iter=241 [00:00:04.595]	 [-]loss: 16711.0597 [00:00:00.234]
Iter=242 [00:00:04.606]	 [-]loss: 16707.3972 [00:00:00.227]
Iter=243 [00:00:04.554]	 [-]loss: 16703.7673 [00:00:00.228]
Iter=244 [00:00:04.609]	 [-]loss: 16700.1696 [00:00:00.235]
Iter=245 [00:00:04.564]	 [-]loss: 16696.6037 [00:00:00.242]
Iter=246 [00:00:04.595]	 [-]loss: 16693.0694 [00:00:00.225]
Iter=247 [00:00:04.616]	 [-]loss: 16689.5663 [00:00:00.252]
Iter=248 [00:00:04.578]	 [-]loss: 16686.0942 [00:00:00.243]
Iter=249 [00:00:04.580]	 [-]loss: 16682.6526 [00:00:00.227]
Iter=250 [00:00:04.587]	 [-]loss: 16679.2414 [00:00:00.231]
Iter=251 [00:00:04.597]	 [-]loss: 16675.8602 [00:00:00.232]
Iter=252 [00:00:04.647]	 [-]loss: 16672.5086 [00:00:00.232]
Iter=253 [00:00:04.586]	 [-]loss: 16669.1864 [00:00:00.229]
Iter=254 [00:00:04.534]	 [-]loss: 16665.8932 [00:00:00.233]
Iter=255 [00:00:04.609]	 [-]loss: 16662.6288 [00:00:00.244]
Iter=256 [00:00:04.669]	 [-]loss: 16659.3928 [00:00:00.252]
Iter=257 [00:00:04.553]	 [-]loss: 16656.1849 [00:00:00.234]
Iter=258 [00:00:04.600]	 [-]loss: 16653.0048 [00:00:00.234]
Iter=259 [00:00:04.587]	 [-]loss: 16649.8521 [00:00:00.227]
Iter=260 [00:00:04.600]	 [-]loss: 16646.7266 [00:00:00.242]
Iter=261 [00:00:04.542]	 [-]loss: 16643.6279 [00:00:00.224]
Iter=262 [00:00:04.562]	 [-]loss: 16640.5557 [00:00:00.226]
Iter=263 [00:00:04.611]	 [-]loss: 16637.5098 [00:00:00.225]
Iter=264 [00:00:04.543]	 [-]loss: 16634.4897 [00:00:00.224]
Iter=265 [00:00:04.534]	 [-]loss: 16631.4953 [00:00:00.238]
Iter=266 [00:00:04.588]	 [-]loss: 16628.5261 [00:00:00.230]
Iter=267 [00:00:04.713]	 [-]loss: 16625.5819 [00:00:00.232]
Iter=268 [00:00:04.640]	 [-]loss: 16622.6625 [00:00:00.226]
Iter=269 [00:00:04.592]	 [-]loss: 16619.7674 [00:00:00.232]
Iter=270 [00:00:04.584]	 [-]loss: 16616.8965 [00:00:00.233]
Iter=271 [00:00:04.518]	 [-]loss: 16614.0494 [00:00:00.230]
Iter=272 [00:00:04.661]	 [-]loss: 16611.2259 [00:00:00.223]
Iter=273 [00:00:04.661]	 [-]loss: 16608.4256 [00:00:00.224]
Iter=274 [00:00:04.711]	 [-]loss: 16605.6484 [00:00:00.224]
Iter=275 [00:00:04.669]	 [-]loss: 16602.8939 [00:00:00.229]
Iter=276 [00:00:04.677]	 [-]loss: 16600.1619 [00:00:00.228]
Iter=277 [00:00:04.543]	 [-]loss: 16597.4521 [00:00:00.222]
Iter=278 [00:00:04.543]	 [-]loss: 16594.7643 [00:00:00.229]
Iter=279 [00:00:04.570]	 [-]loss: 16592.0982 [00:00:00.228]
Iter=280 [00:00:04.547]	 [-]loss: 16589.4536 [00:00:00.229]
Iter=281 [00:00:04.654]	 [-]loss: 16586.8301 [00:00:00.238]
Iter=282 [00:00:04.584]	 [-]loss: 16584.2277 [00:00:00.235]
Iter=283 [00:00:04.726]	 [-]loss: 16581.6460 [00:00:00.252]
Iter=284 [00:00:04.894]	 [-]loss: 16579.0848 [00:00:00.224]
Iter=285 [00:00:04.669]	 [-]loss: 16576.5438 [00:00:00.228]
Iter=286 [00:00:04.658]	 [-]loss: 16574.0229 [00:00:00.237]
Iter=287 [00:00:04.571]	 [-]loss: 16571.5218 [00:00:00.230]
Iter=288 [00:00:04.675]	 [-]loss: 16569.0402 [00:00:00.253]
Iter=289 [00:00:04.620]	 [-]loss: 16566.5781 [00:00:00.240]
Iter=290 [00:00:04.582]	 [-]loss: 16564.1350 [00:00:00.226]
Iter=291 [00:00:04.665]	 [-]loss: 16561.7109 [00:00:00.229]
Iter=292 [00:00:04.643]	 [-]loss: 16559.3054 [00:00:00.232]
Iter=293 [00:00:04.629]	 [-]loss: 16556.9185 [00:00:00.229]
Iter=294 [00:00:04.607]	 [-]loss: 16554.5498 [00:00:00.229]
Iter=295 [00:00:04.704]	 [-]loss: 16552.1992 [00:00:00.231]
Iter=296 [00:00:04.779]	 [-]loss: 16549.8665 [00:00:00.229]
Iter=297 [00:00:04.628]	 [-]loss: 16547.5514 [00:00:00.233]
Iter=298 [00:00:04.601]	 [-]loss: 16545.2538 [00:00:00.226]
Iter=299 [00:00:04.692]	 [-]loss: 16542.9735 [00:00:00.229]
Iter=300 [00:00:04.657]	 [-]loss: 16540.7103 [00:00:00.229]
Iter=301 [00:00:04.601]	 [-]loss: 16538.4640 [00:00:00.235]
Iter=302 [00:00:04.759]	 [-]loss: 16536.2344 [00:00:00.239]
Iter=303 [00:00:04.649]	 [-]loss: 16534.0213 [00:00:00.229]
Iter=304 [00:00:04.583]	 [-]loss: 16531.8246 [00:00:00.228]
Iter=305 [00:00:04.628]	 [-]loss: 16529.6441 [00:00:00.308]
Iter=306 [00:00:04.655]	 [-]loss: 16527.4796 [00:00:00.224]
Iter=307 [00:00:04.892]	 [-]loss: 16525.3309 [00:00:00.277]
Iter=308 [00:00:04.809]	 [-]loss: 16523.1978 [00:00:00.290]
Iter=309 [00:00:04.871]	 [-]loss: 16521.0803 [00:00:00.241]
Iter=310 [00:00:04.883]	 [-]loss: 16518.9781 [00:00:00.282]
Iter=311 [00:00:05.038]	 [-]loss: 16516.8910 [00:00:00.279]
Iter=312 [00:00:05.208]	 [-]loss: 16514.8190 [00:00:00.258]
Iter=313 [00:00:04.643]	 [-]loss: 16512.7618 [00:00:00.228]
Iter=314 [00:00:04.613]	 [-]loss: 16510.7193 [00:00:00.224]
Iter=315 [00:00:04.677]	 [-]loss: 16508.6914 [00:00:00.225]
Iter=316 [00:00:04.649]	 [-]loss: 16506.6779 [00:00:00.288]
Iter=317 [00:00:04.748]	 [-]loss: 16504.6786 [00:00:00.260]
Iter=318 [00:00:04.836]	 [-]loss: 16502.6934 [00:00:00.265]
Iter=319 [00:00:04.872]	 [-]loss: 16500.7222 [00:00:00.286]
Iter=320 [00:00:04.875]	 [-]loss: 16498.7649 [00:00:00.280]
Iter=321 [00:00:04.895]	 [-]loss: 16496.8212 [00:00:00.236]
Iter=322 [00:00:04.578]	 [-]loss: 16494.8910 [00:00:00.235]
Iter=323 [00:00:04.775]	 [-]loss: 16492.9743 [00:00:00.229]
Iter=324 [00:00:04.677]	 [-]loss: 16491.0708 [00:00:00.269]
Iter=325 [00:00:04.715]	 [-]loss: 16489.1805 [00:00:00.233]
Iter=326 [00:00:04.740]	 [-]loss: 16487.3032 [00:00:00.249]
Iter=327 [00:00:04.861]	 [-]loss: 16485.4388 [00:00:00.265]
Iter=328 [00:00:04.660]	 [-]loss: 16483.5872 [00:00:00.223]
Iter=329 [00:00:04.662]	 [-]loss: 16481.7482 [00:00:00.229]
Iter=330 [00:00:04.878]	 [-]loss: 16479.9217 [00:00:00.283]
Iter=331 [00:00:04.779]	 [-]loss: 16478.1075 [00:00:00.262]
Iter=332 [00:00:04.780]	 [-]loss: 16476.3057 [00:00:00.288]
Iter=333 [00:00:04.720]	 [-]loss: 16474.5160 [00:00:00.235]
Iter=334 [00:00:04.717]	 [-]loss: 16472.7382 [00:00:00.250]
Iter=335 [00:00:04.690]	 [-]loss: 16470.9724 [00:00:00.236]
Iter=336 [00:00:04.810]	 [-]loss: 16469.2184 [00:00:00.286]
Iter=337 [00:00:04.918]	 [-]loss: 16467.4761 [00:00:00.306]
Iter=338 [00:00:04.927]	 [-]loss: 16465.7453 [00:00:00.253]
Iter=339 [00:00:04.719]	 [-]loss: 16464.0260 [00:00:00.228]
Iter=340 [00:00:04.624]	 [-]loss: 16462.3180 [00:00:00.236]
Iter=341 [00:00:04.646]	 [-]loss: 16460.6213 [00:00:00.255]
Iter=342 [00:00:04.565]	 [-]loss: 16458.9357 [00:00:00.226]
Iter=343 [00:00:04.586]	 [-]loss: 16457.2611 [00:00:00.229]
Iter=344 [00:00:04.500]	 [-]loss: 16455.5975 [00:00:00.221]
Iter=345 [00:00:04.568]	 [-]loss: 16453.9446 [00:00:00.224]
Iter=346 [00:00:04.561]	 [-]loss: 16452.3025 [00:00:00.233]
Iter=347 [00:00:04.581]	 [-]loss: 16450.6711 [00:00:00.224]
Iter=348 [00:00:04.549]	 [-]loss: 16449.0501 [00:00:00.226]
Iter=349 [00:00:04.564]	 [-]loss: 16447.4396 [00:00:00.227]
Iter=350 [00:00:04.631]	 [-]loss: 16445.8395 [00:00:00.238]
Iter=351 [00:00:04.636]	 [-]loss: 16444.2496 [00:00:00.226]
Iter=352 [00:00:04.715]	 [-]loss: 16442.6699 [00:00:00.242]
Iter=353 [00:00:04.643]	 [-]loss: 16441.1003 [00:00:00.231]
Iter=354 [00:00:04.842]	 [-]loss: 16439.5407 [00:00:00.240]
Iter=355 [00:00:04.826]	 [-]loss: 16437.9910 [00:00:00.241]
Iter=356 [00:00:04.703]	 [-]loss: 16436.4512 [00:00:00.258]
Iter=357 [00:00:04.588]	 [-]loss: 16434.9211 [00:00:00.234]
Iter=358 [00:00:04.667]	 [-]loss: 16433.4006 [00:00:00.232]
Iter=359 [00:00:04.712]	 [-]loss: 16431.8898 [00:00:00.237]
Iter=360 [00:00:04.676]	 [-]loss: 16430.3884 [00:00:00.232]
Iter=361 [00:00:04.677]	 [-]loss: 16428.8965 [00:00:00.240]
Iter=362 [00:00:04.798]	 [-]loss: 16427.4140 [00:00:00.244]
Iter=363 [00:00:04.854]	 [-]loss: 16425.9408 [00:00:00.250]
Iter=364 [00:00:04.846]	 [-]loss: 16424.4767 [00:00:00.227]
Iter=365 [00:00:04.783]	 [-]loss: 16423.0218 [00:00:00.233]
Iter=366 [00:00:04.595]	 [-]loss: 16421.5760 [00:00:00.225]
Iter=367 [00:00:04.532]	 [-]loss: 16420.1392 [00:00:00.224]
Iter=368 [00:00:04.561]	 [-]loss: 16418.7113 [00:00:00.229]
Iter=369 [00:00:04.544]	 [-]loss: 16417.2923 [00:00:00.225]
Iter=370 [00:00:04.567]	 [-]loss: 16415.8820 [00:00:00.235]
Iter=371 [00:00:04.603]	 [-]loss: 16414.4805 [00:00:00.255]
Iter=372 [00:00:04.815]	 [-]loss: 16413.0876 [00:00:00.252]
Iter=373 [00:00:04.942]	 [-]loss: 16411.7034 [00:00:00.226]
Iter=374 [00:00:04.774]	 [-]loss: 16410.3276 [00:00:00.259]
Iter=375 [00:00:04.663]	 [-]loss: 16408.9604 [00:00:00.240]
Iter=376 [00:00:04.799]	 [-]loss: 16407.6015 [00:00:00.227]
Iter=377 [00:00:04.833]	 [-]loss: 16406.2510 [00:00:00.266]
Iter=378 [00:00:04.902]	 [-]loss: 16404.9087 [00:00:00.229]
Iter=379 [00:00:04.732]	 [-]loss: 16403.5747 [00:00:00.233]
Iter=380 [00:00:04.726]	 [-]loss: 16402.2488 [00:00:00.236]
Iter=381 [00:00:04.958]	 [-]loss: 16400.9311 [00:00:00.245]
Iter=382 [00:00:04.797]	 [-]loss: 16399.6214 [00:00:00.246]
Iter=383 [00:00:04.663]	 [-]loss: 16398.3196 [00:00:00.228]
Iter=384 [00:00:04.596]	 [-]loss: 16397.0258 [00:00:00.226]
Iter=385 [00:00:04.571]	 [-]loss: 16395.7399 [00:00:00.226]
Iter=386 [00:00:04.604]	 [-]loss: 16394.4617 [00:00:00.227]
Iter=387 [00:00:04.628]	 [-]loss: 16393.1914 [00:00:00.291]
Iter=388 [00:00:04.624]	 [-]loss: 16391.9287 [00:00:00.223]
Iter=389 [00:00:04.650]	 [-]loss: 16390.6736 [00:00:00.234]
Iter=390 [00:00:04.581]	 [-]loss: 16389.4262 [00:00:00.237]
Iter=391 [00:00:04.565]	 [-]loss: 16388.1862 [00:00:00.225]
Iter=392 [00:00:04.575]	 [-]loss: 16386.9538 [00:00:00.226]
Iter=393 [00:00:04.558]	 [-]loss: 16385.7287 [00:00:00.226]
Iter=394 [00:00:04.637]	 [-]loss: 16384.5111 [00:00:00.240]
Iter=395 [00:00:04.655]	 [-]loss: 16383.3007 [00:00:00.223]
Iter=396 [00:00:04.624]	 [-]loss: 16382.0976 [00:00:00.229]
Iter=397 [00:00:04.593]	 [-]loss: 16380.9017 [00:00:00.227]
Iter=398 [00:00:04.723]	 [-]loss: 16379.7130 [00:00:00.225]
Iter=399 [00:00:04.696]	 [-]loss: 16378.5313 [00:00:00.307]
Iter=400 [00:00:04.735]	 [-]loss: 16377.3568 [00:00:00.233]
Iter=401 [00:00:04.646]	 [-]loss: 16376.1892 [00:00:00.227]
Iter=402 [00:00:04.802]	 [-]loss: 16375.0286 [00:00:00.232]
Iter=403 [00:00:04.706]	 [-]loss: 16373.8748 [00:00:00.247]
Iter=404 [00:00:04.688]	 [-]loss: 16372.7280 [00:00:00.226]
Iter=405 [00:00:04.683]	 [-]loss: 16371.5879 [00:00:00.237]
Iter=406 [00:00:04.549]	 [-]loss: 16370.4547 [00:00:00.235]
Iter=407 [00:00:04.598]	 [-]loss: 16369.3281 [00:00:00.232]
Iter=408 [00:00:04.672]	 [-]loss: 16368.2082 [00:00:00.225]
Iter=409 [00:00:04.715]	 [-]loss: 16367.0949 [00:00:00.239]
Iter=410 [00:00:04.724]	 [-]loss: 16365.9883 [00:00:00.234]
Iter=411 [00:00:04.872]	 [-]loss: 16364.8881 [00:00:00.236]
Iter=412 [00:00:04.944]	 [-]loss: 16363.7944 [00:00:00.261]
Iter=413 [00:00:04.817]	 [-]loss: 16362.7072 [00:00:00.253]
Iter=414 [00:00:04.701]	 [-]loss: 16361.6264 [00:00:00.228]
Iter=415 [00:00:04.664]	 [-]loss: 16360.5520 [00:00:00.226]
Iter=416 [00:00:04.839]	 [-]loss: 16359.4838 [00:00:00.257]
Iter=417 [00:00:04.625]	 [-]loss: 16358.4219 [00:00:00.232]
Iter=418 [00:00:04.589]	 [-]loss: 16357.3663 [00:00:00.264]
Iter=419 [00:00:04.716]	 [-]loss: 16356.3168 [00:00:00.239]
Iter=420 [00:00:04.669]	 [-]loss: 16355.2735 [00:00:00.234]
Iter=421 [00:00:04.732]	 [-]loss: 16354.2362 [00:00:00.224]
Iter=422 [00:00:04.662]	 [-]loss: 16353.2050 [00:00:00.248]
Iter=423 [00:00:04.598]	 [-]loss: 16352.1798 [00:00:00.232]
Iter=424 [00:00:04.615]	 [-]loss: 16351.1606 [00:00:00.233]
Iter=425 [00:00:04.615]	 [-]loss: 16350.1472 [00:00:00.233]
Iter=426 [00:00:04.589]	 [-]loss: 16349.1397 [00:00:00.240]
Iter=427 [00:00:04.675]	 [-]loss: 16348.1380 [00:00:00.229]
Iter=428 [00:00:04.662]	 [-]loss: 16347.1421 [00:00:00.228]
Iter=429 [00:00:04.663]	 [-]loss: 16346.1518 [00:00:00.234]
Iter=430 [00:00:04.655]	 [-]loss: 16345.1673 [00:00:00.251]
Iter=431 [00:00:04.739]	 [-]loss: 16344.1883 [00:00:00.226]
Iter=432 [00:00:04.607]	 [-]loss: 16343.2149 [00:00:00.231]
Iter=433 [00:00:04.657]	 [-]loss: 16342.2471 [00:00:00.230]
Iter=434 [00:00:04.549]	 [-]loss: 16341.2847 [00:00:00.230]
Iter=435 [00:00:04.554]	 [-]loss: 16340.3277 [00:00:00.233]
Iter=436 [00:00:04.593]	 [-]loss: 16339.3761 [00:00:00.232]
Iter=437 [00:00:04.656]	 [-]loss: 16338.4298 [00:00:00.239]
Iter=438 [00:00:04.580]	 [-]loss: 16337.4888 [00:00:00.228]
Iter=439 [00:00:04.738]	 [-]loss: 16336.5531 [00:00:00.305]
Iter=440 [00:00:04.776]	 [-]loss: 16335.6225 [00:00:00.224]
Iter=441 [00:00:04.611]	 [-]loss: 16334.6971 [00:00:00.232]
Iter=442 [00:00:04.641]	 [-]loss: 16333.7768 [00:00:00.227]
Iter=443 [00:00:04.656]	 [-]loss: 16332.8615 [00:00:00.258]
Iter=444 [00:00:04.605]	 [-]loss: 16331.9513 [00:00:00.251]
Iter=445 [00:00:04.765]	 [-]loss: 16331.0460 [00:00:00.232]
Iter=446 [00:00:04.620]	 [-]loss: 16330.1457 [00:00:00.227]
Iter=447 [00:00:04.679]	 [-]loss: 16329.2502 [00:00:00.240]
Iter=448 [00:00:04.674]	 [-]loss: 16328.3597 [00:00:00.238]
Iter=449 [00:00:04.657]	 [-]loss: 16327.4739 [00:00:00.244]
Iter=450 [00:00:04.681]	 [-]loss: 16326.5929 [00:00:00.238]
Iter=451 [00:00:05.119]	 [-]loss: 16325.7167 [00:00:00.286]
Iter=452 [00:00:04.838]	 [-]loss: 16324.8452 [00:00:00.223]
Iter=453 [00:00:04.755]	 [-]loss: 16323.9783 [00:00:00.247]
Iter=454 [00:00:04.848]	 [-]loss: 16323.1161 [00:00:00.239]
Iter=455 [00:00:04.864]	 [-]loss: 16322.2585 [00:00:00.237]
Iter=456 [00:00:04.801]	 [-]loss: 16321.4054 [00:00:00.230]
Iter=457 [00:00:04.758]	 [-]loss: 16320.5569 [00:00:00.229]
Iter=458 [00:00:04.670]	 [-]loss: 16319.7128 [00:00:00.266]
Iter=459 [00:00:04.738]	 [-]loss: 16318.8732 [00:00:00.272]
Iter=460 [00:00:04.859]	 [-]loss: 16318.0380 [00:00:00.252]
Iter=461 [00:00:04.717]	 [-]loss: 16317.2072 [00:00:00.230]
Iter=462 [00:00:04.804]	 [-]loss: 16316.3807 [00:00:00.251]
Iter=463 [00:00:04.753]	 [-]loss: 16315.5586 [00:00:00.281]
Iter=464 [00:00:04.871]	 [-]loss: 16314.7407 [00:00:00.227]
Iter=465 [00:00:04.819]	 [-]loss: 16313.9270 [00:00:00.246]
Iter=466 [00:00:04.881]	 [-]loss: 16313.1175 [00:00:00.231]
Iter=467 [00:00:04.838]	 [-]loss: 16312.3123 [00:00:00.260]
Iter=468 [00:00:04.920]	 [-]loss: 16311.5111 [00:00:00.249]
Iter=469 [00:00:04.854]	 [-]loss: 16310.7141 [00:00:00.295]
Iter=470 [00:00:04.749]	 [-]loss: 16309.9211 [00:00:00.273]
Iter=471 [00:00:04.659]	 [-]loss: 16309.1321 [00:00:00.256]
Iter=472 [00:00:04.872]	 [-]loss: 16308.3472 [00:00:00.238]
Iter=473 [00:00:04.733]	 [-]loss: 16307.5662 [00:00:00.239]
Iter=474 [00:00:04.745]	 [-]loss: 16306.7892 [00:00:00.243]
Iter=475 [00:00:04.667]	 [-]loss: 16306.0160 [00:00:00.227]
Iter=476 [00:00:04.621]	 [-]loss: 16305.2468 [00:00:00.229]
Iter=477 [00:00:04.835]	 [-]loss: 16304.4814 [00:00:00.243]
Iter=478 [00:00:04.852]	 [-]loss: 16303.7199 [00:00:00.227]
Iter=479 [00:00:04.743]	 [-]loss: 16302.9621 [00:00:00.226]
Iter=480 [00:00:04.635]	 [-]loss: 16302.2081 [00:00:00.273]
Iter=481 [00:00:04.801]	 [-]loss: 16301.4578 [00:00:00.264]
Iter=482 [00:00:04.659]	 [-]loss: 16300.7113 [00:00:00.228]
Iter=483 [00:00:04.779]	 [-]loss: 16299.9684 [00:00:00.225]
Iter=484 [00:00:05.174]	 [-]loss: 16299.2292 [00:00:00.338]
Iter=485 [00:00:05.086]	 [-]loss: 16298.4937 [00:00:00.298]
Iter=486 [00:00:05.180]	 [-]loss: 16297.7617 [00:00:00.230]
Iter=487 [00:00:04.798]	 [-]loss: 16297.0333 [00:00:00.225]
Iter=488 [00:00:04.752]	 [-]loss: 16296.3085 [00:00:00.235]
Iter=489 [00:00:04.635]	 [-]loss: 16295.5872 [00:00:00.233]
Iter=490 [00:00:04.659]	 [-]loss: 16294.8695 [00:00:00.239]
Iter=491 [00:00:04.695]	 [-]loss: 16294.1552 [00:00:00.227]
Iter=492 [00:00:04.550]	 [-]loss: 16293.4443 [00:00:00.226]
Iter=493 [00:00:04.580]	 [-]loss: 16292.7369 [00:00:00.223]
Iter=494 [00:00:04.572]	 [-]loss: 16292.0329 [00:00:00.225]
Iter=495 [00:00:04.734]	 [-]loss: 16291.3323 [00:00:00.240]
Iter=496 [00:00:04.640]	 [-]loss: 16290.6351 [00:00:00.229]
Iter=497 [00:00:04.583]	 [-]loss: 16289.9411 [00:00:00.229]
Iter=498 [00:00:04.594]	 [-]loss: 16289.2505 [00:00:00.229]
Iter=499 [00:00:04.606]	 [-]loss: 16288.5632 [00:00:00.228]
MF_extALS	 <hr, ndcg, prec>:	 0.1931	 0.0471	 0.0161 [00:40:58.699]
*/